1.overall GD:When using Gradient Descent, you should ensure that all features
have a similar scale (e.g., using Scikit-Learn’s StandardScaler
class), or else it will take much longer to converge.

2.Draw back of linear algorithm:Both the Normal Equation and the SVD approach get very slow
when the number of features grows large (e.g., 100,000). On the
positive side, both are linear with regards to the number of instan‐
ces in the training set (they are O(m)), so they handle large training
sets efficiently, provided they can fit in memory.

3.Batch GD:When the cost function is convex and its slope does not change abruptly (as is the
case for the MSE cost function), Batch Gradient Descent with a fixed learning rate
will eventually converge to the optimal solution, but you may have to wait a while: it
can take O(1/ε) iterations to reach the optimum within a range of ε depending on the
shape of the cost function. If you divide the tolerance by 10 to have a more precise
solution, then the algorithm may have to run about 10 times longer.

4.Stoc-hasting:Chooses random point,train on huge training sets, since only one instance needs to be in memory at each iteration Stochastic Gradient Descent has a better
chance of finding the global minimum than Batch Gradient Descent does.When using Stochastic Gradient Descent, the training instances
must be independent and identically distributed (IID), to ensure
that the parameters get pulled towards the global optimum, on
average. A simple way to ensure this is to shuffle the instances dur‐
ing training (e.g., pick each instance randomly, or shuffle the train‐
ing set at the beginning of each epoch). If you do not do this, for
example if the instances are sorted by label, then SGD will start by
optimizing for one label, then the next, and so on, and it will not
settle close to the global minimum.

5.Mini-Batch: Performance boost from hardware optimization of matrix operations, especially
when using GPUs.It has less persentage to stuck in local minima compared to Full_Batch.
So, mini_batch for faster computation, whereas sgd's speed depends on faith, as it work 
on random point.

6.Polynomial Regression:Note that when there are multiple features, Polynomial Regression is capable of find‐
ing relationships between features (which is something a plain Linear Regression
model cannot do).PolynomialFeatures(degree=d) transforms an array containing n
features into an array containing (n + d)! / (d!n!)
features, where n! is the
factorial of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐
rial explosion of the number of features!

7.The Bias/Variance Tradeof:

An important theoretical result of statistics and Machine Learning is the fact that a
model’s generalization error can be expressed as the sum of three very different
errors:
Bias
This part of the generalization error is due to wrong assumptions, such as assum‐
ing that the data is linear when it is actually quadratic. A high-bias model is most
likely to underfit the training data.10
Variance
This part is due to the model’s excessive sensitivity to small variations in the
training data. A model with many degrees of freedom (such as a high-degree pol‐
ynomial model) is likely to have high variance, and thus to overfit the training
data.
Irreducible error
This part is due to the noisiness of the data itself. The only way to reduce this
part of the error is to clean up the data (e.g., fix the data sources, such as broken
sensors, or detect and remove outliers).
Increasing a model’s complexity will typically increase its variance and reduce its bias.
Conversely, reducing a model’s complexity increases its bias and reduces its variance.
This is why it is called a tradeoff.


8.Reducing overfitting:a good way to reduce overfitting is to regularize the
model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be
for it to overfit the data. For example, a simple way to regularize a polynomial model
is to reduce the number of polynomial degrees.i

9. Ways to speed up training process:
So far we have seenfour ways to speed up training (and reach a better solution): applying a good initiali‐
zation strategy for the connection weights, using a good activation function, using
Batch Normalization, and reusing parts of a pretrained network (possibly built on an
auxiliary task or using unsupervised learning). Another huge speed boost comes from
using a faster optimizer than the regular Gradient Descent optimizer.

10. Default DNN setting:

use. The configuration in Table 11-2 will work fine in
most cases, without requiring much hyperparameter tuning.
Table 11-2. Default DNN coniguration
Hyperparameter Default value
Kernel initializer: LeCun initialization
Activation function: SELU
Normalization: None (self-normalization)
Regularization: Early stopping
Optimizer: Nadam
Learning rate schedule: Performance scheduling

The default configuration in Table 11-2 may need to be tweaked:

• If your model self-normalizes:
— If it overfits the training set, then you should add alpha dropout (and always
use early stopping as well). Do not use other regularization methods, or else
they would break self-normalization.
• If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip
connections):
— You can try using ELU (or another activation function) instead of SELU, it
may perform better. Make sure to change the initialization method accord‐
ingly (e.g., He init for ELU or ReLU).
— If it is a deep network, you should use Batch Normalization after every hidden
layer. If it overfits the training set, you can also try using max-norm or l2
reg‐

ularization.
• If you need a sparse model, you can use l1

regularization (and optionally zero out
the tiny weights after training). If you need an even sparser model, you can try
using FTRL instead of Nadam optimization, along with l1

regularization. In any
case, this will break self-normalization, so you will need to switch to BN if your
model is deep.
• If you need a low-latency model (one that performs lightning-fast predictions),
you may need to use less layers, avoid Batch Normalization, and possibly replace
the SELU activation function with the leaky ReLU. Having a sparse model will
also help. You may also want to reduce the float precision from 32-bits to 16-bit
(or even 8-bits) (see ???).
• If you are building a risk-sensitive application, or inference latency is not very
important in your application, you can use MC Dropout to boost performance
and get more reliable probability estimates, along with uncertainty estimates.
