{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bfb0b6e-c4f8-432a-9a47-04bdc2506a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 17:16:28.258239: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-22 17:16:28.446074: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 17:16:28.446117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 17:16:28.448692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 17:16:28.468519: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-22 17:16:28.469721: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 17:16:30.475415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d16a60-1993-4891-84ba-2b56b66c399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32fb2c94-aace-455c-a461-697fdf3df899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66195e4f-6a29-435a-ac37-b061e99210ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "keras.layers.Dense(10, activation=\"softmax\")\n",
    "model.compile(optimizer= keras.optimizers.SGD(learning_rate=3e-3), loss = keras.losses.sparse_categorical_crossentropy, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"batch_normal_logs\", \"run{:03d}\".format(run_index))\n",
    "cb = keras.callbacks.TensorBoard(run_logdir), keras.callbacks.EarlyStopping(patience=1)\n",
    "#model.fit(X_train, y_train, epochs=10,\n",
    "         # validation_data=(X_valid, y_valid),\n",
    "         # callbacks=[cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76806028-8ae8-4cc5-86af-8f64a088de3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (2, 3)\n",
      "Shape with new axis after rows: (1, 2, 3)\n",
      "Shape with new axis before columns: (2, 3, 1)\n",
      "Shape with selected element and new axis: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a sample tensor\n",
    "t = tf.constant([[1, 2, 3], \n",
    "                 [4, 5, 6]])\n",
    "\n",
    "# Print original shape\n",
    "print(\"Original shape:\", t.shape)  # Output: (2, 3)\n",
    "\n",
    "# Insert a new axis after the first axis (rows)\n",
    "new_axis_after_rows = t[tf.newaxis, ...]\n",
    "print(\"Shape with new axis after rows:\", new_axis_after_rows.shape)  # Output: (1, 2, 3)\n",
    "\n",
    "# Insert a new axis before the last axis (columns)\n",
    "new_axis_before_columns = t[:, :, tf.newaxis]\n",
    "print(\"Shape with new axis before columns:\", new_axis_before_columns.shape)  # Output: (2, 3, 1)\n",
    "\n",
    "# Select the second element along the last axis and create a new axis\n",
    "selected_and_new_axis = t[..., 1, tf.newaxis]\n",
    "print(\"Shape with selected element and new axis:\", selected_and_new_axis.shape)  # Output: (2, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd064f-6ac2-4d59-8d83-912c1f86d4e7",
   "metadata": {},
   "source": [
    "# Some tensor operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a088fdbf-c214-4dd6-b183-c1a0ed813bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(1.) + tf.constant(1, dtype=tf.float32) # Tensorflow always use float32 bit to so operaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a87ca43-fada-4b85-9886-33d24a035714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(1.) + tf.constant(1, dtype=tf.float64)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c484f2a-15d3-4c52-bb93-4415dce75bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5a6e45-1a33-4387-9f5c-4f84363868bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "195bf470-f24e-4b2d-bfbd-8b9d0f6b3ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 2,  4,  6],\n",
       "       [ 8, 10, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(v * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4310153-7b31-41fa-9aef-a820f4075932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 2, 42,  6],\n",
       "       [ 8, 10, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0,1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2828ab4-d44e-4d04-b61a-601cb36879b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[100,  42,   6],\n",
       "       [  8,  10, 200]], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices = [[0,0], [1,2]], updates=[100,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "820577a4-e32a-498e-a248-e7c091b6ebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[ 0, 42,  6],\n",
       "       [ 8, 10,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_sub(indices = [[0,0], [1,2]], updates=[100,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea26e0-e6d0-48a5-bb26-f7026c5cd1ca",
   "metadata": {},
   "source": [
    "# Tensor's data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48014b93-f427-4ee9-b430-1da76a970b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'• Sparse tensors (tf.SparseTensor) efficiently represent tensors containing mostly\\n0s. The tf.sparse package contains operations for sparse tensors.\\n• Tensor arrays (tf.TensorArray) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n• Ragged tensors (tf.RaggedTensor) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged package contains\\noperations for ragged tensors.\\n• String tensors are regular tensors of type tf.string. These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"café\"`), then it will get encoded to\\nUTF-8 automatically (e.g., b\"cafÃ©\"). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32, where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233]). The tf.strings package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n• Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n• Queues, including First In, First Out (FIFO) queues (FIFOQueue), queues that can\\nprioritize some items (PriorityQueue), queues that shuffle their items (Random\\nShuffleQueue), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue). These classes are all in the tf.queue package.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"• Sparse tensors (tf.SparseTensor) efficiently represent tensors containing mostly\n",
    "0s. The tf.sparse package contains operations for sparse tensors.\n",
    "• Tensor arrays (tf.TensorArray) are lists of tensors. They have a fixed size by\n",
    "default, but can optionally be made dynamic. All tensors they contain must have\n",
    "the same shape and data type.\n",
    "• Ragged tensors (tf.RaggedTensor) represent static lists of lists of tensors, where\n",
    "every tensor has the same shape and data type. The tf.ragged package contains\n",
    "operations for ragged tensors.\n",
    "• String tensors are regular tensors of type tf.string. These actually represent byte\n",
    "strings, not Unicode strings, so if you create a string tensor using a Unicode\n",
    "string (e.g., a regular Python 3 string like \"café\"`), then it will get encoded to\n",
    "UTF-8 automatically (e.g., b\"caf\\xc3\\xa9\"). Alternatively, you can represent\n",
    "Unicode strings using tensors of type tf.int32, where each item represents a\n",
    "Unicode codepoint (e.g., [99, 97, 102, 233]). The tf.strings package (with\n",
    "an s) contains ops for byte strings and Unicode strings (and to convert one into\n",
    "the other).\n",
    "• Sets are just represented as regular tensors (or sparse tensors) containing one or\n",
    "more sets, and you can manipulate them using operations from the tf.sets\n",
    "package.\n",
    "• Queues, including First In, First Out (FIFO) queues (FIFOQueue), queues that can\n",
    "prioritize some items (PriorityQueue), queues that shuffle their items (Random\n",
    "ShuffleQueue), and queues that can batch items of different shapes by padding\n",
    "(PaddingFIFOQueue). These classes are all in the tf.queue package.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f07ffa-163a-41f7-a821-16ba5964377a",
   "metadata": {},
   "source": [
    "# [Author's examples](https://github.com/Computerlo/handson-ml2/blob/master/12_custom_models_and_training_with_tensorflow.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2463da4-16d7-4edf-8e23-de893034c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the author's examples as his examples are very weightful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0f718-5836-413a-95de-86ea9d951173",
   "metadata": {},
   "source": [
    "# Sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c1d18d3-e646-41f8-8a4e-73f6be82c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0 10  0  0]\n",
      " [ 0  0 20  0]\n",
      " [ 0  0  0  0]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "indices = [[0, 1], [1, 2]]\n",
    "values = [10, 20]\n",
    "sparse_tensor = tf.sparse.SparseTensor(indices, values, dense_shape=[3, 4])\n",
    "\n",
    "# Accessing values\n",
    "print(tf.sparse.to_dense(sparse_tensor))  # Output: [[0 10 0 0] [0 0 20 0] [0 0 0 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c625332-80b9-44ac-9241-997b9478d59c",
   "metadata": {},
   "source": [
    "# Tensor Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66e2dc1b-cd7b-4664-a127-bacbeafcd629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], shape=(3, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "array = tf.TensorArray(tf.float32, size=3)\n",
    "array = array.write(0, tf.constant([1.0, 2.0]))\n",
    "array = array.write(1, tf.constant([3.0, 4.0]))\n",
    "array = array.write(2, tf.constant([5.0, 6.0]))\n",
    "\n",
    "# Reading values\n",
    "print(array.stack())  # Output: tf.Tensor([[1. 2.] [3. 4.] [5. 6.]])\n",
    "array.read(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebca64-1ea9-4ad6-be8f-bee15133757e",
   "metadata": {},
   "source": [
    "# Ragged tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bae3d0d-e254-48cd-98ca-6fd8ac168665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1, 2, 3], [], [4, 5]]>\n",
      "tf.Tensor([], shape=(0,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ragged_tensor = tf.ragged.constant([[1, 2, 3], [], [4, 5]])\n",
    "\n",
    "# Accessing values\n",
    "print(ragged_tensor)  # Output: <tf.RaggedTensor [[1, 2, 3], [], [4, 5]]>\n",
    "print(ragged_tensor[1])  # Output: <tf.RaggedTensor []>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c9751-bf5f-43a6-8520-5af8dda41030",
   "metadata": {},
   "source": [
    "# String tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894edd27-3d4d-4e23-af88-b6aceb934ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'hello' b'world' b'caf\\xc3\\xa9'], shape=(3,), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "string_tensor = tf.constant([\"Hello\", \"world\", \"café\"])\n",
    "\n",
    "# Converting to lowercase\n",
    "print(tf.strings.lower(string_tensor))  # Output: tf.Tensor([b'hello' b'world' b'caf\\xc3\\xa9'])\n",
    "u = tf.constant([ord(c) for c in \"café\"]) # ord(c) is used to obtain its Unicode code point.\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90d6d995-3746-4f08-9734-aa46a1681550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.strings.unicode_encode(u, \"UTF-8\")\n",
    "tf.strings.length(b, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40fe1d9a-26a1-4d7d-971a-3c88babcf3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(b, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8397253-31e0-4fb0-aa5b-7caa52f70252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101],\n",
       " [99, 97, 102, 102, 232], [21654, 21857]]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])\n",
    "\n",
    "tf.strings.length(p, unit=\"UTF8_CHAR\") # Output: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)>\n",
    "\n",
    "r = tf.strings.unicode_decode(p, \"UTF8\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef55110-dce3-4f59-94bf-8e10bd783eac",
   "metadata": {},
   "source": [
    "# Set tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8787b4db-8245-4646-b243-71b324cfea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [1 1]], shape=(3, 2), dtype=int64), values=tf.Tensor([1 3 4], shape=(3,), dtype=int32), dense_shape=tf.Tensor([2 2], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "set1 = tf.constant([[1, 2], [3, 4]])\n",
    "set2 = tf.constant([[2, 3], [5, 6]])\n",
    "\n",
    "# Set difference\n",
    "print(tf.sets.difference(set1, set2))  # Output: tf.Tensor([[1], [4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f38389-5e62-46dd-9159-4e8e7c81c5e8",
   "metadata": {},
   "source": [
    "# Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "534cca0b-4483-436f-ac17-5191837fadc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber loss is used in the case of dataset with outliers\n",
    "# keras.loss.Huber() is the huber function\n",
    "\n",
    "# Here is a custom huber function\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ddbcbad-44c5-4a2f-a2fd-fdadc4a878a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\n",
    "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a656796f-5514-4035-be8d-dd986ac60b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 4s 5ms/step - loss: 0.6130 - mse: 2.3288 - val_loss: 0.2851 - val_mse: 11.3806\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2101 - mse: 0.5758 - val_loss: 0.2486 - val_mse: 5.9304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5b913c7e20>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=keras.activations.selu, kernel_initializer=keras.initializers.lecun_normal,\n",
    "                      input_shape = input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = huber_fn, optimizer=\"nadam\", metrics=[\"mse\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24f9e1-b701-4f76-b2b1-020c774bd306",
   "metadata": {},
   "source": [
    "# Saving and Loading Models That Contain Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df2b00c4-48f7-49d1-addf-eaaf967d01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arman/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 0.2009 - mse: 0.4878 - val_loss: 0.2075 - val_mse: 1.4604\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.1960 - mse: 0.4598 - val_loss: 0.1838 - val_mse: 0.4314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5b8ca93cd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"custom_func_model.h5\")\n",
    "model = keras.models.load_model(\"custom_func_model.h5\",\n",
    "                                custom_objects={\"huber_fn\":huber_fn}) # we have to specify the function name\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, \n",
    "         validation_data = (X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff2a392a-78ff-4d2a-90f7-58d3680a23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the case of diffrent file i have to run from \"file_name\" import huber_fn(function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce08cdae-be29-4ac9-b63e-8d6cb221b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: -1.4680 - mse: 0.4587 - val_loss: -1.0187 - val_mse: 1.3750\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: -1.4783 - mse: 0.4479 - val_loss: -1.4864 - val_mse: 0.4417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5b9123ec50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_huber(threshold = 1.0):  # For applying different threshold we make an outter func\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        small_error = tf.abs(error) > threshold\n",
    "        squared_error = tf.square(error)/2\n",
    "        linear_error = tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(small_error, squared_error, linear_error)\n",
    "    return huber_fn\n",
    "name = \"model_with_custom_func_and_threshold.h5\"\n",
    "model.save(name)\n",
    "model = keras.models.load_model(name,\n",
    "                               custom_objects={\"huber_fn\": create_huber(2.0)}) # We have to provide the parameter always\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "         validation_data =(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9306529-1c19-4778-a39f-05d40c4ce981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: -0.2074 - mae: 1.0014 - val_loss: 7.5831 - val_mae: 0.6098\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: -1.3363 - mae: 0.5138 - val_loss: 4.4789 - val_mae: 0.5663\n",
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: -1.4001 - mae: 0.4977 - val_loss: 1.7789 - val_mae: 0.5335\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: -1.4331 - mae: 0.4886 - val_loss: 0.3014 - val_mae: 0.5121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5b90891e10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HuberLoss(keras.losses.Loss): # Solve the problem of always giving a parameter on loading a model, a class can be created\n",
    "    def __init__(self, threshold = 1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        small_error = tf.abs(error) > self.threshold\n",
    "        squared_error = tf.square(error)/2\n",
    "        linear_error = tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(small_error, squared_error, linear_error)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config,\"threshold\":self.threshold}\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
    "\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
    "                                custom_objects={\"HuberLoss\": HuberLoss})  # <------- no need to initialize the parameter.\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e804583-2265-49cf-b048-a44b9380a70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f356dc5-8bec-46cd-aeb6-04088036606e",
   "metadata": {},
   "source": [
    "# Custom Activation Functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "311ea4ac-b198-433b-99d9-05d52cd284b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32): # same as keras.initializers.glorot_normal\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights): # same as keras.regularizers.l1\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "024592a9-f620-4082-8c47-0a8af4ed62cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 1.7965 - mae: 0.9428 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.6014 - mae: 0.5190 - val_loss: inf - val_mae: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arman/.local/lib/python3.10/site-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
      "  warnings.warn(\n",
      "/home/arman/.local/lib/python3.10/site-packages/keras/src/regularizers.py:426: UserWarning: The `keras.regularizers.serialize()` API should only be used for objects of type `keras.regularizers.Regularizer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
      "  warnings.warn(\n",
      "/home/arman/.local/lib/python3.10/site-packages/keras/src/constraints.py:365: UserWarning: The `keras.constraints.serialize()` API should only be used for objects of type `keras.constraints.Constraint`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model = keras.models.load_model(\"my_model_with_many_custom_parts.h5\",\\n    custom_objects={\\n       \"my_l1_regularizer\": my_l1_regularizer,\\n       \"my_positive_weights\": my_positive_weights,\\n       \"my_glorot_initializer\": my_glorot_initializer,\\n       \"my_softplus\": my_softplus,\\n    })\\n\\nfor some reason this part is not working for me so I commented it'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "\n",
    "\"\"\"model = keras.models.load_model(\"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })\n",
    "\n",
    "for some reason this part is not working for me so I commented it\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "036492ba-f35e-4b84-9529-ee73bcd0b54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 1.6786 - mae: 0.9561 - val_loss: 1.4523 - val_mae: 0.6162\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6413 - mae: 0.5577 - val_loss: 1.1309 - val_mae: 0.5311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5b90fee230>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44088cce-b170-4c82-97ff-7de1c2b9ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you must implement the call() method for losses, layers (including activa‐\n",
    "# tion functions) and models, or the __call__() method for regularizers, initializers\n",
    "# and constraints.\n",
    "# call() can be used but __call__() is considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bc9d9-8698-4648-963b-407890bc10d8",
   "metadata": {},
   "source": [
    "# Custom metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ff30e64-e458-48e6-8bee-8b83346ae8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\") # Used the custom function to calculate loss\n",
    "\n",
    "# Here is custom metric subclass to calculate huber loss\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(self.threshold)\n",
    "        self.total = self.add_weight('total',initializer='zeros') # Createa variable called total initialize the value 0\n",
    "        self.count = self.add_weight('count',initializer='zeros')\n",
    "    def update_state(self, y_true, y_pred):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric)) # Calculates the sum of elements within the metric tensor,\n",
    "                                                     # which contains the Huber loss values for the current batch.\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return { ** base_config, \"treshold\":self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc727fd3-d2ae-40d6-a889-bad7509f3a77",
   "metadata": {},
   "source": [
    "# Custom layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f113d1-25fd-4ff6-9415-434281f001de",
   "metadata": {},
   "source": [
    "#### Weightless layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7bfe2b8-7e20-4672-af74-b5b42b8e6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create weight less layer like .Flatten and .Input use .Lambda function\n",
    "\n",
    "# Here is a layer that outputs the exp of the input\n",
    "exp_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "\n",
    "# The exponential layer is sometimes\n",
    "# used in the output layer of a regression model when the values to predict have very\n",
    "# different scales (e.g., 0.001, 10., 1000.).\n",
    "\n",
    "\n",
    "\n",
    "# Model with exponential output layer for regression with varying scales\n",
    "\"\"\"model = keras.Sequential([\n",
    "    # ... other layers\n",
    "    \"\"\"\"\"\"exponential_layer\"\"\"\"\"\"  # Apply exponential function to output\n",
    "])\"\"\"\n",
    "\n",
    "\n",
    "# Define a custom activation function\n",
    "def my_activation(x):\n",
    "    return tf.maximum(x, 0.2 * x + 0.5)  # Example non-linear function\n",
    "\n",
    "# Create a Lambda layer using the custom activation\n",
    "activation_layer = keras.layers.Lambda(my_activation)\n",
    "\n",
    "# Use it in a model\n",
    "\"\"\"model = keras.Sequential([\n",
    "    # ... other layers\n",
    "    \"\"\"\"\"\"activation_layer\"\"\"\"\"\"  # Apply custom activation\n",
    "])\"\"\"\n",
    "\n",
    "\n",
    "# Concatenate outputs of two layers using a Lambda layer\n",
    "\"\"\"merged_layer = keras.layers.Lambda(lambda x: tf.concat([x[0], x[1]], axis=1))([layer1, layer2])\"\"\"\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Lambda\n",
    "\n",
    "input_tensor = Input(shape=(10,))  # Input layer\n",
    "\n",
    "layer1 = Dense(32, activation='relu')(input_tensor)  # First dense layer\n",
    "layer2 = Dense(16, activation='relu')(input_tensor)  # Second dense layer\n",
    "\n",
    "# Concatenate outputs of layer1 and layer2 using a Lambda layer\n",
    "merged_layer = Lambda(lambda x: tf.concat([x[0], x[1]], axis=1))([layer1, layer2])\n",
    "\n",
    "output_layer = Dense(1, activation='sigmoid')(merged_layer)  # Output layer\n",
    "\n",
    "model = keras.Model(inputs=input_tensor, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1f225d7-0220-4aa6-b1ab-ae2253ed1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)# Keras provides a wide range of activation functions, and\n",
    "                                                            # keras.activations.get allows the code  to work with any of them dynamically.\n",
    "\n",
    "    def build(self, batch_input_shape): # Keras will know the number of the layer's input\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d121b372-cc49-4e88-b496-9607f97d5f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 1.4760 - val_loss: 0.7477\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6304 - val_loss: 0.5251\n",
      "162/162 [==============================] - 0s 3ms/step - loss: 0.5411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5411323308944702"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c33e58f6-0410-4419-8dd1-c26aa525cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model_with_a_custom_layer.h5\")\n",
    "\n",
    "# model = keras.models.load_model(\"my_model_with_a_custom_layer.h5\",\n",
    "#                                 custom_objects={\"MyDense\": MyDense})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad93a395-e780-4067-ab91-f25ef2711974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:  (None, 2)  X2.shape:  (None, 2)\n"
     ]
    }
   ],
   "source": [
    "# A layers on multioutput\n",
    "\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        print(\"X1.shape: \", X1.shape ,\" X2.shape: \", X2.shape) # Debugging of custom layer\n",
    "        return X1 + X2, X1 * X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape2]\n",
    "\n",
    "\n",
    "inputs1 = keras.layers.Input(shape=[2])\n",
    "inputs2 = keras.layers.Input(shape=[2])\n",
    "outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e81556ec-b2fd-453f-9ab5-71bf525158e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For different behaviour in layer like: Batch_noma..., Dropout etc.\n",
    "# a training parameter need to be added in the call() method\n",
    "\n",
    "# the following layers adds gaussina to the during computation\n",
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    def call(self, X, training = None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e71dc-e4a7-47b6-abcf-27a924a6ac7d",
   "metadata": {},
   "source": [
    "# Custom model(with residual block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ee8912f-e135-4dbd-a4e9-6e7a2b63d9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEach residual block contain:\\n^\\n|\\n+\\n^\\n|\\nDense\\n^\\n|\\nDense\\n^\\n|\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following model will have this architecture\n",
    "# Dense\n",
    "# Residual Block   \n",
    "\"\"\"\n",
    "Each residual block contain:\n",
    "^\n",
    "|\n",
    "+\n",
    "^\n",
    "|\n",
    "Dense\n",
    "^\n",
    "|\n",
    "Dense\n",
    "^\n",
    "|\n",
    "\"\"\"\n",
    "# 4 x (Resisual Block)\n",
    "# Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f167476-a03b-4279-87e5-817b9336f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First a class for the residual block\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self,n_layers, n_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_units, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "458ac07c-c89e-4db8-b36b-cb5ac6f05bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the model with the 3 looped residual block\n",
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.hidden1 =keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\") # First layer\n",
    "        self.block1 = ResidualBlock(2,30) # 2 layers and 30 units\n",
    "        self.block2 = ResidualBlock(2,30)\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3): # 3 loop and 1 for itself\n",
    "            Z = self.block1(Z)\n",
    "        self.block2(Z)\n",
    "        return self.out(Z)  # This is the output layer\n",
    "\n",
    "\n",
    "# For model.save functionality {{def get_config method need to added in the model}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bff62b7d-881a-49ff-8972-f20f2d623285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['residual_regressor/residual_block_1/dense_4/kernel:0', 'residual_regressor/residual_block_1/dense_4/bias:0', 'residual_regressor/residual_block_1/dense_5/kernel:0', 'residual_regressor/residual_block_1/dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['residual_regressor/residual_block_1/dense_4/kernel:0', 'residual_regressor/residual_block_1/dense_4/bias:0', 'residual_regressor/residual_block_1/dense_5/kernel:0', 'residual_regressor/residual_block_1/dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['residual_regressor/residual_block_1/dense_4/kernel:0', 'residual_regressor/residual_block_1/dense_4/bias:0', 'residual_regressor/residual_block_1/dense_5/kernel:0', 'residual_regressor/residual_block_1/dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['residual_regressor/residual_block_1/dense_4/kernel:0', 'residual_regressor/residual_block_1/dense_4/bias:0', 'residual_regressor/residual_block_1/dense_5/kernel:0', 'residual_regressor/residual_block_1/dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "363/363 [==============================] - 5s 5ms/step - loss: 2.0469\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.8513\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5100\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6259\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5282\n",
      "162/162 [==============================] - 1s 3ms/step - loss: 0.5705\n",
      "162/162 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_new_scaled = X_test_scaled\n",
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26a60e9d-16d1-4b61-9fa6-97148fee7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 6s 5ms/step - loss: 0.8742\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5312\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6767\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4169\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4043\n",
      "162/162 [==============================] - 1s 3ms/step - loss: 0.5310\n",
      "162/162 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# The following will do the ResidualRegressor class's work using Sequential\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "block1 = ResidualBlock(2, 30)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1, block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbdc229f-8aa6-4f8a-9276-8b14644b01bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe Model class is a subclass of the Layer class, so models can be defined and used\\nexactly like layers. But a model has some extra functionalities, including of course its\\ncompile(), fit(), evaluate(), and predict() methods (and a few variants), plus the\\nget_layers() method (which can return any of the model’s layers by name or by\\nindex) and the save() method (and support for keras.models.load_model() and\\nkeras.models.clone_model()).\\n\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Model class is a subclass of the Layer class, so models can be defined and used\n",
    "exactly like layers. But a model has some extra functionalities, including of course its\n",
    "compile(), fit(), evaluate(), and predict() methods (and a few variants), plus the\n",
    "get_layers() method (which can return any of the model’s layers by name or by\n",
    "index) and the save() method (and support for keras.models.load_model() and\n",
    "keras.models.clone_model()).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330414dc-8bda-484f-91a9-ac8a14d044b2",
   "metadata": {},
   "source": [
    "# Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afa08a93-73eb-4b7a-a9c5-e87775957ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 7s 6ms/step - loss: 0.7630 - reconstruction_error: 0.7586\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4007 - reconstruction_error: 0.3311\n",
      "162/162 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        #super().build(batch_input_shape), in 2.2 update it do not work for model\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z) # An auxiliary layer with different output\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss) # This ensures that the reconstruction loss does not dominate the main loss.\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z) # main output\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7efc6234-5d3e-4d5e-915e-e05cdb4098e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nyou can add a custom metric based on model internals by computing it in\\nany way you want, as long as the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean object in the constructor, then call it in the\\ncall() method, passing it the recon_loss, and finally add it to the model by calling\\nthe model’s add_metric() method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch.\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "you can add a custom metric based on model internals by computing it in\n",
    "any way you want, as long as the result is the output of a metric object. For example,\n",
    "you can create a keras.metrics.Mean object in the constructor, then call it in the\n",
    "call() method, passing it the recon_loss, and finally add it to the model by calling\n",
    "the model’s add_metric() method. This way, when you train the model, Keras will\n",
    "display both the mean loss over each epoch (the loss is the sum of the main loss plus\n",
    "0.05 times the reconstruction loss) and the mean reconstruction error over each\n",
    "epoch.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0d0e2-6cfd-4cea-b315-ca50f19d537c",
   "metadata": {},
   "source": [
    "# Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "961e4a4b-66b1-42aa-a8f4-3d582bc0294c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2]) # Derifate Z with respect to w1 and w2\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e760022e-f693-4f59-acb9-8f9f3cc41880",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape: # persistance for using it multiple times\n",
    "    z = f(w1,w2)\n",
    "\n",
    "gradients1 = tape.gradient(z,w1)\n",
    "gradients2 = tape.gradient(z,w2)\n",
    "del tape # Must del it or it will keep storing values of the previous calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da00c5b0-61f3-4dd0-9ff8-2bcc110ad827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients # Constant can not be used for tape only Variable(this is default setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fe6bf37-6e8d-4b61-97bc-08ddda3c62f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1) # Now it will calculate for the constant values too\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c223882-57f7-4a35-a7be-efa4ff9688fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "tape.gradient([z1, z2, z3], [w1, w2])\n",
    "\n",
    "# Use like this for simpler equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64efe3a4-e434-4c40-bee3-90da0c188c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([136.,  30.], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "a = tf.reduce_sum(tf.stack([tape.gradient(z, [w1, w2]) for z in (z1, z2, z3)]), axis=0)\n",
    "del tape\n",
    "a\n",
    "\n",
    "# use like this for complex equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c622959-49fe-446e-a6de-e307e4b1fa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n",
      "[[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>], [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, None]]\n"
     ]
    }
   ],
   "source": [
    "# Jacob and hessian\n",
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = f(w1, w2)\n",
    "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
    "hessians = [hessian_tape.gradient(jacobian, [w1, w2])\n",
    "            for jacobian in jacobians]\n",
    "del hessian_tape\n",
    "\n",
    "print(jacobians)\n",
    "print(hessians)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24662e96-b649-4811-b0dd-75aedde7dc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "x = tf.Variable([100.]) # With greater value it returns nan cause it can not claculate with inf. value\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "gradient = tape.gradient(z,[x])\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d58cb6f7-d834-40f2-bf7c-0cbe09cd183f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([inf], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients\n",
    "\n",
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5e69bdd-124c-4281-96f9-cdcbbc424859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1000.], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_better_softplus(z):\n",
    "    return tf.where(z > 30., z, tf.math.log(tf.exp(z) + 1.))\n",
    "\n",
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "189c0845-4b5a-444c-a255-2c0503c61219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading... |░░░░░░░░░░░░░░░░░░░░░░░░░░░░░-| 99.0% Complete\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def progress_bar(iteration, total, size=30, prefix='Progress:', suffix='Complete', fill='█', print_end=\"\\r\"):\n",
    "    \"\"\"\n",
    "    Create a visual progress bar with customizable features.\n",
    "    \"\"\"\n",
    "\n",
    "    percent = (\"{0:.1f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(size * iteration // total)\n",
    "    bar = fill * filled_length + '-' * (size - filled_length)\n",
    "\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end=print_end)\n",
    "    if iteration == total:  # Print a newline at the end\n",
    "        print()\n",
    "\n",
    "# Example usage:\n",
    "for i in range(100):\n",
    "    time.sleep(0.1)  # Simulate a task taking time\n",
    "    progress_bar(i, 100, prefix='Downloading...', suffix='Complete', fill='░')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce4a6291-2e0b-4b40-ab84-9b44504b11be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████| 100.0% Complete - loss: 0.0900 - mean_square: 858.5000"
     ]
    }
   ],
   "source": [
    "def progress_bar(iteration, total, size=30, prefix='Progress:', suffix='Complete', fill='█'):\n",
    "    \"\"\"\n",
    "    Create a visual progress bar.\n",
    "\n",
    "    Args:\n",
    "        iteration (int): Current iteration number.\n",
    "        total (int): Total number of iterations.\n",
    "        size (int, optional): Length of the progress bar. Defaults to 30.\n",
    "        prefix (str, optional): Text to display before the progress bar. Defaults to 'Progress:'.\n",
    "        suffix (str, optional): Text to display after the progress bar. Defaults to 'Complete'.\n",
    "        fill (str, optional): Character to use for filling the bar. Defaults to '█'.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted progress bar string.\n",
    "    \"\"\"\n",
    "\n",
    "    percent = (\"{0:.1f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(size * iteration // total)\n",
    "    bar = fill * filled_length + '-' * (size - filled_length)\n",
    "    return f'\\r{prefix} |{bar}| {percent}% {suffix}'\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30, prefix='Progress:', suffix='Complete', fill='█'):\n",
    "    \"\"\"\n",
    "    Print a combined progress bar and status bar.\n",
    "\n",
    "    Args:\n",
    "        iteration (int): Current iteration number.\n",
    "        total (int): Total number of iterations.\n",
    "        loss (float): Current loss value.\n",
    "        metrics (list, optional): List of other metrics to display. Defaults to None.\n",
    "        size (int, optional): Length of the progress bar. Defaults to 30.\n",
    "        prefix (str, optional): Text to display before the progress bar. Defaults to 'Progress:'.\n",
    "        suffix (str, optional): Text to display after the progress bar. Defaults to 'Complete'.\n",
    "        fill (str, optional): Character to use for filling the bar. Defaults to '█'.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics_str = \" - \".join([f\"{m.name}: {m.result():.4f}\" for m in [loss] + (metrics or [])])\n",
    "    print(f\"\\r{progress_bar(iteration, total, size, prefix, suffix, fill)} - {metrics_str}\", end=\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a86af03d-e8bb-47b6-ab17-797780cda170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='elu', kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(.05)),\n",
    "    keras.layers.Dense(1, kernel_regularizer=keras.regularizers.l2(.05))\n",
    "])\n",
    "\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a61f533-75af-4289-a3e7-904267ee6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf87440a-5ad6-42b2-a2ed-ad4bb61c3818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Progress: |██████████████████████████████| 100.0% Complete - mean: 1.4279 - mean_absolute_error: 0.5913Epoch 2/5\n",
      "Progress: |██████████████████████████████| 100.0% Complete - mean: 0.6644 - mean_absolute_error: 0.5289Epoch 3/5\n",
      "Progress: |██████████████████████████████| 100.0% Complete - mean: 0.6321 - mean_absolute_error: 0.5198Epoch 4/5\n",
      "Progress: |██████████████████████████████| 100.0% Complete - mean: 0.6431 - mean_absolute_error: 0.5225Epoch 5/5\n",
      "Progress: |██████████████████████████████| 100.0% Complete - mean: 0.6624 - mean_absolute_error: 0.5299"
     ]
    }
   ],
   "source": [
    "\"\"\"If you add weight constraints to your model (e.g., by setting kernel_constraint or\n",
    "bias_constraint when creating a layer), you should update the training loop to\n",
    "apply these constraints just after apply_gradients():\"\"\"\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09148d7f-77b5-41df-9512-8bc88c0a738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Training loop with training flag:\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below are some other custom implementation for the custom training loop\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Using Adam optimizer with clipnorm:\n",
    "\"\"\"\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.0)\n",
    "\n",
    "\"\"\"\n",
    "Training loop with gradient clipping:\n",
    "\"\"\"\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     ...\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         ...\n",
    "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\"\"\"\n",
    "        Clip gradients before applying updates:\n",
    "\"\"\"\n",
    "    # clipped_gradients = [tf.clip_by_norm(g, clip_norm=1.0) for g in gradients]\n",
    "    #     optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "    # ...\n",
    "\n",
    "\"\"\"\n",
    "Using clipvalue with any optimizer:\n",
    "\"\"\"\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "\"\"\"\n",
    "Training loop with clipvalue:\n",
    "\"\"\"\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     ...\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         ...\n",
    "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "\"\"\"\n",
    "        Clip individual gradient absolute values before applying updates:\n",
    "\"\"\"\n",
    "    #     clipped_gradients = [tf.clip_by_value(g, -0.5, 0.5) for g in gradients]\n",
    "    #     optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "    # ...\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Scaling gradients by a factor:\n",
    "\"\"\"\n",
    "# def scale_gradients(gradients, factor):\n",
    "#     return [g * factor for g in gradients]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training loop with scaled gradients:\n",
    "\"\"\"\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     ...\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         ...\n",
    "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "\"\"\"\n",
    "        Apply custom scaling before updates:\n",
    "\"\"\"\n",
    "    #     scaled_gradients = scale_gradients(gradients, 0.8)\n",
    "    #     optimizer.apply_gradients(zip(scaled_gradients, model.trainable_variables))\n",
    "    # ...\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Setting training mode for BatchNormalization and Dropout layers:\n",
    "\"\"\"\n",
    "# model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " Training loop with training flag:\n",
    "\"\"\"\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     ...\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         y_pred = model(X_batch, training=True)  # Set training=True for these layers\n",
    "#         ...\n",
    "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bee46f-04e8-41db-aaf3-cc10e475e082",
   "metadata": {},
   "source": [
    "# Tensorflow function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "749b4e68-c4a9-431c-ae3e-0451775efb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92cca572-31f0-433e-b670-ddd126636581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f5b937933a0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube) # turning in to tersor function\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ad5f0e6-89de-4b19-8b14-5dcff69c7bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2) #The passed value is tensor like this one(e.g. 2,3,10...) tf will alwaya create a new graph\n",
    "           #So this value should be used as constant\n",
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2a2f980-bfaa-48fc-ac3a-b9b2d7d1fb3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def tf__cube(x):\\n    with ag__.FunctionScope('cube', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\\n        do_return = False\\n        retval_ = ag__.UndefinedReturnValue()\\n        try:\\n            do_return = True\\n            retval_ = ag__.ld(x) ** 3\\n        except:\\n            do_return = False\\n            raise\\n        return fscope.ret(retval_, do_return)\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.autograph.to_code(tf_cube.python_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24e6d35d-a5a1-4c66-b9a3-6cd0d6dad499",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x**3\n",
    "\n",
    "# This is same as previous tf_cube but it is one step process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f77d8-71bc-44dc-a5f6-88985c463908",
   "metadata": {},
   "source": [
    "# Some tesorflow function rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2efd5619-6994-428a-aacd-305d3caf340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_function(x):\n",
    "  return np.sum(x)\n",
    "\n",
    "f1 = bad_function(tf.constant([1, 2]))\n",
    "f2 = bad_function(tf.constant([3, 4]))\n",
    "\n",
    "# Both f1 and f2 will return the same random value since np.sum runs only during tracing.\n",
    "\n",
    "def good_function(x):\n",
    "  return tf.reduce_sum(x)\n",
    "\n",
    "g1 = good_function(tf.constant([1, 2]))\n",
    "g2 = good_function(tf.constant([3, 4]))\n",
    "\n",
    "# g1 and g2 will return different sums as tf.reduce_sum is part of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3469507-f3de-4040-9183-2e0a4b6082b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_function(x):\n",
    "  return tf.math.log(x)\n",
    "\n",
    "def main_function(x):\n",
    "  return helper_function(x) * 2\n",
    "\n",
    "# Both helper_function and main_function operations are included in the graph.\n",
    "# TensorFlow can capture operations of other Python functions called within the TF function, \n",
    "# even if they aren't decorated with @tf.function.\n",
    "# These functions also follow the TF function rules for consistent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ac29995a-ab19-4599-9b35-81e9a32b2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad practice: creating and assigning within the function\n",
    "\n",
    "def bad_function():\n",
    "  v = tf.Variable(0.0)\n",
    "  v = v + 1.0  # This assignment won't be part of the graph.\n",
    "\n",
    "# Good practice: creating outside and assigning inside\n",
    "\n",
    "v = tf.Variable(0.0)\n",
    "\n",
    "def good_function():\n",
    "  v.assign_add(1.0)  # This assignment is included in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f80171e-0acc-42e6-8ad9-e1ed01d0690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_function(x):\n",
    "  y = 0\n",
    "  for i in range(x):\n",
    "    y += i  # This loop won't be part of the graph.\n",
    "\n",
    "def good_function(x):\n",
    "  y = 0\n",
    "  for i in tf.range(x):\n",
    "    y += i  # This loop is included in the graph and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dfeaea08-8d17-498c-86a3-81ee7af65da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def bad_function(x):\n",
    "  # Local variable only incremented during tracing\n",
    "  local_counter = 0\n",
    "  y = tf.square(x)\n",
    "  local_counter += 1\n",
    "  # Print counter, only shows 1 regardless of calls\n",
    "  tf.print(local_counter)\n",
    "  return y\n",
    "\n",
    "# Calling function multiple times doesn't reflect local counter increments\n",
    "f1 = bad_function(2)\n",
    "f2 = bad_function(3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def good_function(x):\n",
    "#   # Counter object defined outside for shared state\n",
    "#   global global_counter\n",
    "#   global_counter = 0\n",
    "#   y = tf.square(x)\n",
    "#   # Increment with tf.py_function within the graph\n",
    "#   with tf.py_function(lambda: global_counter.incr(), [], []):\n",
    "#     pass\n",
    "#   # Print counter, reflects actual increments\n",
    "#   tf.print(global_counter)\n",
    "#   return y\n",
    "\n",
    "# # Calling function increments the shared counter\n",
    "# f1 = good_function(2)\n",
    "# f2 = good_function(3)\n",
    "\n",
    "# # global_counter should now be 2\n",
    "# print(global_counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6ba95-ab90-478f-8a9f-3ad6b040d346",
   "metadata": {},
   "source": [
    "# Some rules to make python function convertible to tensorflow function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1381754-8f52-4227-8d6a-925aa66cbe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_layer(x):\n",
    "  # Use TensorFlow operations like activation and convolution\n",
    "  y = tf.nn.relu(x)\n",
    "  y = tf.nn.conv2d(y, W, strides=2, padding=\"SAME\")\n",
    "  # Return a TensorFlow tensor\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d627f6c-f120-4bd0-9759-908387ac4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "  # Avoid loops and conditionals, use TensorFlow logic\n",
    "  error = tf.abs(y_true - y_pred)\n",
    "  # Use TensorFlow reduction instead of loops\n",
    "  loss = tf.reduce_mean(error)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5fc0ebd-28b7-4742-9cb8-ccc23b566978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function used within the TF function\n",
    "def my_external_function(x):\n",
    "  # ... some non-TensorFlow logic ...\n",
    "  return result\n",
    "\n",
    "def custom_model(x):\n",
    "  # Declare `my_external_function` as a dependency\n",
    "  my_ext_result = tf.compat.v1.executing_eagerly_outside_functions(my_external_function, x)\n",
    "  # Use Tensorflow operations with the result\n",
    "  y = tf.multiply(x, my_ext_result)\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "308d9ceb-fcf0-4793-b6eb-b3ebf873e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMomentumOptimizer(keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.9, name=\"MyMomentumOptimizer\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
    "        self._set_hyper(\"decay\", self._initial_decay) # \n",
    "        self._set_hyper(\"momentum\", momentum)\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"momentum\")\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
    "        momentum_var = self.get_slot(var, \"momentum\")\n",
    "        momentum_hyper = self._get_hyper(\"momentum\", var_dtype)\n",
    "        momentum_var.assign(momentum_var * momentum_hyper - (1. - momentum_hyper)* grad)\n",
    "        var.assign_add(momentum_var * lr_t)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
