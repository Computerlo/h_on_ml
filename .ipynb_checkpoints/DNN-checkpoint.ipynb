{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377a3f83-0386-485e-add9-a9d0e04cace1",
   "metadata": {},
   "source": [
    "# How to set initilizer in deep neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f698c9e5-b792-425a-b3a4-5594c44e5112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 04:20:53.747030: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-18 04:20:53.940996: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-18 04:20:53.941051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-18 04:20:53.943601: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-18 04:20:53.960879: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-18 04:20:53.961740: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-18 04:20:55.916927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x7fccb8baded0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "he_initialization = keras.initializers.he_normal\n",
    "keras.layers.Dense(30, activation = 'relu',kernel_initializer=he_initialization)\n",
    "#or\n",
    "keras.layers.Dense(30, activation = 'relu',kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f038a-76d0-49b1-ba03-847d2924e1f9",
   "metadata": {},
   "source": [
    "# If we want to initialize fan_avg in he_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529372a7-2709-481c-8e7a-c72a53f23e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x7fccb8d1acb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(30, activation='relu',kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cfc6c4-69fe-4df5-8570-60f599317ae3",
   "metadata": {},
   "source": [
    "# Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed78a20c-2295-466d-88e9-32d965be0443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(), # Leaky Relu is used like this separately which is why I wrote ir an example\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b8872-dbb9-404b-88ca-8e3ef91b355a",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa6838a-64fd-49b4-a6cf-757d718a760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, use_bias=False), # Deactivating bias is very common in BN as it uses offset\n",
    "    keras.layers.BatchNormalization(momentum=.99, axis=-1), # 2 parameter that can be tweaked see the axis explanation below\n",
    "    keras.layers.Activation('relu'), # Using activation func after BN will normalize the input for next layer\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "]) # This just an example not a model, just to show case the batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92329437-7be7-49d3-b0f4-d72c2a90789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=-1 (channel-wise normalization): This is the most common setting in 2D convolutional neural networks (CNNs). \n",
    "#          It tells the layer to perform normalization within each image,independently for each channel. \n",
    "#          For instance, it would calculate the mean and variance of all whisker pixel values across all cat images in the batch\n",
    "#          and then normalize each cat's whisker values based on those statistics. \n",
    "#          This ensures that whiskers from different cats are compared fairly even if they have differing brightness or contrast.\n",
    "# axis=1 (also channel-wise, but in 3D CNNs): This works similarly to axis=-1 but is used in 3D CNNs,\n",
    "#          where your data might have additional dimensions like time or depth.\n",
    "# axis=None (all-axis normalization): This is less common but tells the layer to perform normalization across all dimensions\n",
    "#          of the input data, treating it as a single entity. This might be suitable for certain specialized situations but often\n",
    "#          isn't used in typical image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07fbda0-bd09-4ff5-bb66-2097920a17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24c8659-f5c1-4392-8acb-2b1dc342ff0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_1/gamma:0', True),\n",
       " ('batch_normalization_1/beta:0', True),\n",
       " ('batch_normalization_1/moving_mean:0', False),\n",
       " ('batch_normalization_1/moving_variance:0', False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables] # By non trainable it means parameters that will not be twiked during backpropagation\n",
    "                                                     # for example std and mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae0fe9-1456-45c9-8fca-405ee5974fe1",
   "metadata": {},
   "source": [
    "# Graddient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621c6ea3-e74b-4acc-9a7b-38ea1dff69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipping used to solve gradient explotion by setting an threshold for gradient vector\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ee60d-d538-4692-a579-6780554c737a",
   "metadata": {},
   "source": [
    "# Tranfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a46d776-1f90-49e5-8357-daa2d93c35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model('best_model.h5')\n",
    "# Let's say I want to build another model_B using the same neurons of the A model.\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # Expect for the outpur layer so we typed -1\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid')) # output layer of the B model\n",
    "\n",
    "\n",
    "# We are using the A model's neuron layer in our new model for boolean classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a2ce9-e914-47b8-abcc-ecef667566e5",
   "metadata": {},
   "source": [
    "# Cloning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccc753f-5147-48c7-ab3c-6c8ac4f857b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to train model B explicitely on a dataset, we have to clone it so changes do not effect the model A's neurons\n",
    "\n",
    "model_A_clone = keras.models.clone_model(model_A) # This code effectively copies the learned knowledge (weights and biases) from model A\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8b147-b182-449f-920d-2ec4e34bb035",
   "metadata": {},
   "source": [
    "# Freezing some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d048bb-480e-4a48-841d-47ab04f79285",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False # Freezing all the neurons that are from the other model\n",
    "\n",
    "# model.compile and model.fit to train the non reused neurons then\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25993bfc-f39c-40d2-a7b1-cdfb6ff64b5c",
   "metadata": {},
   "source": [
    "# Updating learning rate at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06b2b90-c030-43da-a79c-1fbc125dcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to update the learning rate at each iteration rather than at each epoch, \n",
    "# you must write your own callback class:\n",
    "from keras.datasets import boston_housing\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: the `batch` argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.learning_rate)\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr * 0.1**(1 / self.s))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.learning_rate)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "lr0 = 0.01\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=lr0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "\n",
    "exp_decay = ExponentialDecay()\n",
    "# history = model.fit(epochs=n_epochs,\n",
    "#                     validation_data=(X_valid_scaled, y_valid),\n",
    "#                     callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffd352cc-bc3b-41f8-9a73-25aa903cd869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 6ms/step - loss: 2.3573 - accuracy: 0.1110\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.3164 - accuracy: 0.1310\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.2923 - accuracy: 0.1350\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.2767 - accuracy: 0.1520\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.2624 - accuracy: 0.1540\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.2529 - accuracy: 0.1610\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.2435 - accuracy: 0.1630\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.2349 - accuracy: 0.1720\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 2.2268 - accuracy: 0.1740\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.2204 - accuracy: 0.1780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fcc3c656440>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "# Create a simple model with two Dense layers\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,), kernel_constraint=max_norm(3)),\n",
    "    Dense(10, activation='softmax', kernel_constraint=max_norm(2))\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate some sample data\n",
    "X_train = tf.random.normal((1000, 10))\n",
    "y_train = tf.one_hot(tf.random.uniform((1000,), minval=0, maxval=10, dtype=tf.int32), depth=10)\n",
    "\n",
    "# Train the model (max-norm constraint is enforced during training)\n",
    "model.fit(X_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5258ab1-8d17-4160-bca7-d2f08e529b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.08, 0.08, 0.11, 0.13, 0.11, 0.09, 0.1 , 0.09, 0.1 , 0.1 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "mc_model.set_weights(model.get_weights())\n",
    "\n",
    "\n",
    "np.round(np.mean([mc_model.predict(X_train[:1]) for sample in range(100)], axis=0), 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
